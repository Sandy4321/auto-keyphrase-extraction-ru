{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Preprocess text extracted from scholarly articles into keyphrase candidates.       \n",
    "\n",
    "The expected input is a list of text extracted from scholarly papers published in Russian from\n",
    "the dialog conference. The first step is to remove the reference section and any unusually short\n",
    "paragraphs. Then, parenthesized and bracketed text are removed, as well as any words that happen\n",
    "to be cut off at a new page boundary.\n",
    "\n",
    "The text is then tokenized using the preprocessing and tokenization technique developed by \n",
    "WebVectores [Kutuzov 2017]. At this point, documents in the corpus are transformed into\n",
    "lists of tokens corresponding to the paragraphs in the document. Tokens are unigrams of lowercased\n",
    "lemmas or punctuation marks.\n",
    "\n",
    "Each document is then broken into sentences, and n-grams are added. This results in each document\n",
    "being transformed into a list of sentence tokens, which may be unigrams, bigrams, or trigrams.\n",
    "Finally, the sentence tokens are filtered to remove any unusually short or long tokens and stop\n",
    "words. The document is flattened, resulting in it being transformed into a list of keyphrase \n",
    "candidate tokens.\n",
    "\n",
    "Typical usage example:\n",
    "\n",
    "    file_names = get_file_names(\"../data/raw_txt/2010/ru\", \".txt\")\n",
    "    corpus = [get_text(name) for name in file_names]\n",
    "    corpus = ScholarlyPreprocessor.preprocess(corpus, file_names)\n",
    "\"\"\"\n",
    "\n",
    "# Reset the kernel (ipython magic command)\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading the model...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Whyve\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Phrases\n",
    "from helpers import get_file_names, get_text\n",
    "from itertools import groupby\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import logging\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import rus_preprocessing_udpipe\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "class ScholarlyPreprocessor(object):\n",
    "    \"\"\"Prepares a list of raw Russian text from scholarly parpers into a list of normalized \n",
    "    tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    russian_stopwords = stopwords.words(\"russian\") + [\"который\", \"это\", \"также\"]\n",
    "    is_sentence_delimiter = re.compile(r'^[\\.!?\\n]$')\n",
    "    \n",
    "    # Regular expressions for filtering out from text\n",
    "    references_section = re.compile(r'Литература$|Список литературы$', re.MULTILINE)\n",
    "    parentheses_or_brackets = re.compile(r'\\(.*?\\)|\\[.*?\\]', re.DOTALL)\n",
    "    word_at_pagebreak = re.compile(r'([А-яЁё\\w]+-[\\n$])|\\n([а-яёa-z]+)')\n",
    "    \n",
    "    # Regular expressions for filtering tokens to keep\n",
    "    length_threshold = re.compile(r'^.{3,100}$')\n",
    "    has_alphabetical = re.compile(r'\\w*?[А-яЁёA-Za-z]\\w*')\n",
    "    has_alphabetical_or_is_sentence_delimiter = re.compile(r'{}|{}'.format(\n",
    "        has_alphabetical.pattern, is_sentence_delimiter.pattern))\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def remove_references_section(cls, text: str, file_name: Optional[Path] = None) -> str:\n",
    "        \"\"\"Remove any test past the last line that ends in either 'Литература' \n",
    "        or 'Список литературы'.\n",
    "        \n",
    "        Args:\n",
    "            text:\n",
    "                The full text of a scholarly paper.\n",
    "            file_name: \n",
    "                Optional; The path to the file that contains the text that is being\n",
    "                preprocessed.\n",
    "\n",
    "        Returns:\n",
    "            The text of a scholarly paper except for the references section. \n",
    "        \"\"\"\n",
    "        # Get the last match\n",
    "        match = None\n",
    "        for match in re.finditer(cls.references_section, text):\n",
    "            pass\n",
    "\n",
    "        if match == None:\n",
    "            logging.info(\"Could not find references section\" +\n",
    "                         (\"\" if (file_name == None) else (\" for file: \" + str(file_name))))\n",
    "        else:\n",
    "            text = text[:match.span()[0]]\n",
    "        return text\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def remove_short_paragraphs(cls, text: str, min_paragraph_length: int = 100):\n",
    "        \"\"\"Remove any paragraphs that are shorter than a specified length.\n",
    "        \n",
    "        Args:\n",
    "            text:\n",
    "                The full text of a scholarly paper.\n",
    "            min_paragraph_length:\n",
    "                Optional; The threshold for deciding how long a paragraph\n",
    "                must be to not be removed. The default is to remove paragraphs shorter than\n",
    "                100 chars.\n",
    "        \n",
    "        Returns:\n",
    "            The text of a scholarly paper except for any paragraphs shorter than a specified\n",
    "            length\n",
    "        \"\"\"\n",
    "        paragraphs = [\" \".join(paragraph.split(\"\\n\")) for paragraph in text.split(\"\\n\\n\")]\n",
    "        paragraphs = [paragraph for paragraph in paragraphs \n",
    "                      if len(paragraph) >= min_paragraph_length]\n",
    "        return '\\n'.join(paragraphs)\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def filter_text(cls, text: str, patterns: List[re.Pattern]) -> str:\n",
    "        \"\"\"Filter substrings from a text based on a list of regular expressions.\n",
    "        \n",
    "        Args:\n",
    "            text: \n",
    "                The text to filter.\n",
    "            patterns: \n",
    "                A list of compiled regex patterns to match.\n",
    "            \n",
    "        Returns:\n",
    "            Filtered text that doesn't match the list of regular expressions.\n",
    "        \"\"\"\n",
    "        filtered_text = text\n",
    "        for pattern in patterns:\n",
    "            filtered_text = re.sub(pattern, \"\", filtered_text)\n",
    "        return filtered_text\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def tokenize(cls, text: str, keep_pos: bool = False, \n",
    "                 keep_punct: bool = True) -> List[List[str]]:\n",
    "        \"\"\"Lemmatize and tokenize a text paragraph by paragraph with udpipe.\n",
    "        \n",
    "        Args:\n",
    "            text:\n",
    "                The text to tokenize. Paragraphs are delimited by a single newline character\n",
    "            keep_pos:\n",
    "                Optional; Append the part of speech tag to the end of each token or\n",
    "                not. The default behavior is to drop the POS tag.\n",
    "            keep_punct:\n",
    "                Optional; Keep punctuation marks as separate tokens. The default\n",
    "                behavior is to keep punctuation.\n",
    "        \n",
    "        Returns:\n",
    "            A list of each paragraph, where each paragraph is a list of lemmatized tokens. \n",
    "        \"\"\"\n",
    "        tokens: List[List[str]] = [rus_preprocessing_udpipe.process(\n",
    "            rus_preprocessing_udpipe.process_pipeline, text=paragraph,\n",
    "            keep_pos=keep_pos, keep_punct=keep_punct) + ['\\n']\n",
    "                  for paragraph in text.split('\\n')]\n",
    " \n",
    "        return tokens\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def lowercase_tokens(cls, tokens: List[str]) -> List[str]:\n",
    "        \"\"\"Convert any tokens to its lowercase equivalent.\n",
    "        \n",
    "        Args:\n",
    "            text: \n",
    "                The list of tokens to lowercase.\n",
    "        \n",
    "        Returns:\n",
    "            A list of lowercase tokens.\n",
    "        \"\"\"\n",
    "        return [token.lower() for token in tokens]\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def filter_tokens(cls, tokens: List[str], patterns: List[re.Pattern]) -> List[str]:\n",
    "        \"\"\"Filter out tokens that don't match a list of regular expressions.\n",
    "        \n",
    "        Args:\n",
    "            tokens:\n",
    "                The list of tokens to filter.\n",
    "            patterns: \n",
    "                A list of compiled regex patterns to match.\n",
    "        \n",
    "        Returns:\n",
    "            A list of tokens which match the list of regular expressions.\n",
    "        \"\"\"\n",
    "        filtered_tokens = tokens\n",
    "        for pattern in patterns:\n",
    "            filtered_tokens = [token for token in filtered_tokens\n",
    "                               if re.search(pattern, token)]\n",
    "        return filtered_tokens\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def remove_stop_words(cls, tokens: List[str], stop: List[str]) -> List[str]:\n",
    "        \"\"\"Remove frequently appearing words from a text.\n",
    "        \n",
    "        Args:\n",
    "            tokens:\n",
    "                The list of tokens from which to remove stop words.\n",
    "            stop:\n",
    "                The list of frequently appearing words.\n",
    "        \n",
    "        Returns:\n",
    "            The list of tokens minus all lone stop words.\n",
    "        \"\"\"\n",
    "        return [token for token in tokens \n",
    "                if not token in stop]\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def get_ngrams(cls, tokens: List[List[str]], n: int = 2, min_count: int = 3,\n",
    "                   delimiter: str = b' ', stop: Optional[List[str]] = None) -> List[List[str]]:\n",
    "        \"\"\"Add up to tri-grams to a list of tokens.\n",
    "        \n",
    "        Args:\n",
    "            tokens:\n",
    "                The list of paragraph tokens from which to search for ngrams.\n",
    "            n:\n",
    "                Optional, either '2' or '3'; Up to bigrams or trigrams. The default is to\n",
    "                add up to bigrams.\n",
    "            min_count: \n",
    "                Optional; The minimum amount of occurances for an ngram to be \n",
    "                added. The default is to add ngrams that occur at least 3 times.\n",
    "            delimiter:\n",
    "                Optional; The byte string to separate words in an n-gram. The\n",
    "                default is to separate words in an n-gram with a space.\n",
    "            stop:\n",
    "                Optional; A list of stop words.\n",
    "        \n",
    "        Returns:\n",
    "            A list of sentence tokens plus ngrams.\n",
    "        \"\"\"\n",
    "        # Break down the list of paragraph tokens into a list of sentences tokens\n",
    "        tokens = [token for paragraph in tokens for token in paragraph]\n",
    "        sentences = [list(token) for delimiter, token in\n",
    "                     groupby(tokens, lambda token: re.match(cls.is_sentence_delimiter, token))\n",
    "                     if not delimiter]\n",
    "        amt_sentences = len(sentences)\n",
    "        \n",
    "        # Find the bigrams\n",
    "        bigram = Phrases(sentences, min_count=min_count, delimiter=delimiter,\n",
    "                        common_terms=stop)\n",
    "\n",
    "        if n == 3:\n",
    "            # Find the trigrams\n",
    "            trigram = Phrases(bigram[sentences], min_count=1, delimiter=delimiter,\n",
    "                             common_terms=stop)\n",
    "            for sentence in range(amt_sentences):\n",
    "                sentences[sentence] = [n_gram for n_gram in trigram[bigram[sentences[sentence]]]]\n",
    "        else:\n",
    "            for sentence in range(amt_sentences):\n",
    "                sentences[sentence] = [n_gram for n_gram in bigram[sentences[sentence]]]\n",
    "                \n",
    "        return sentences\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def preprocess_one(cls, document: str, file_name: Optional[Path] = None, \n",
    "                       verbose: bool = False) -> List[str]:\n",
    "        \"\"\"Preprocess a single raw scholarly text into keyphrase candidates.\n",
    "        \n",
    "        Args:\n",
    "            document:\n",
    "                The document to preprocess\n",
    "            file_name:\n",
    "                Optional; The name of the file that contains the document.\n",
    "            verbose:\n",
    "                Optional; If true, prints out the document after each preprocessing step. The\n",
    "                default behavior is to not do this.\n",
    "        \n",
    "        Returns:\n",
    "            A list of candidate keyphrases from the document.\n",
    "        \"\"\"\n",
    "        if (verbose):\n",
    "            print(\"-\"*50, \"\\nRaw text, before any preprocessing:\\n\\n\")\n",
    "            [[print(line) for line in paragraph.split(\"\\n\")]\n",
    "             for paragraph in document.split(\"\\n\\n\")]\n",
    "            print(\"\\n\\n\", \"-\"*50)\n",
    "            \n",
    "        document = cls.remove_references_section(document, file_name)\n",
    "        document = cls.remove_short_paragraphs(document)\n",
    "\n",
    "        # Remove any words at page breaks, bracketed text, and parethesized text\n",
    "        document = cls.filter_text(document, [cls.parentheses_or_brackets, cls.word_at_pagebreak])\n",
    "        if verbose:\n",
    "            print(\"\\nAfter removing the references section, short paragraphs, \"\n",
    "                  \"parenthesized/bracketed text, and text at pagebreaks:\\n\\n\")\n",
    "            [print(line) for line in document.split('\\n')]\n",
    "            print(\"\\n\\n\", \"-\"*50)\n",
    "        \n",
    "        # After tokenization, the document is represented as a list of paragraph tokens,\n",
    "        # where each paragraph is a list of tokens\n",
    "        document: List[List[str]] = cls.tokenize(document)\n",
    "        if verbose:\n",
    "            print(\"\\nAfter tokenization:\\n\\n\")\n",
    "            [print(line) for line in document]\n",
    "            print(\"\\n\\n\", \"-\"*50)\n",
    "        \n",
    "        document = [cls.filter_tokens(paragraph, [cls.has_alphabetical_or_is_sentence_delimiter])\n",
    "                    for paragraph in document]\n",
    "        if verbose:\n",
    "            print(\"\\nAfter removing tokens which do not have at least 1 alphabetical character \"\n",
    "                  \"and are not sentence delimiters:\\n\\n\")\n",
    "            [print(line) for line in document]\n",
    "            print(\"\\n\\n\", \"-\"*50)\n",
    "        \n",
    "        # After getting n-grams, the document is represented as a list of sentence tokens,\n",
    "        # where each sentence is a list of ngrams.\n",
    "        document = cls.get_ngrams(document, n=3, stop=cls.russian_stopwords)\n",
    "        if verbose:\n",
    "            print(\"\\nAfter adding bigrams and trigrams:\\n\\n\")\n",
    "            [print(sentence) for sentence in document]\n",
    "            print(\"\\n\\n\", \"-\"*50)\n",
    "        \n",
    "        document = [cls.filter_tokens(sentence, [cls.length_threshold])\n",
    "                    for sentence in document]\n",
    "        document = [cls.remove_stop_words(sentence, cls.russian_stopwords)\n",
    "                    for sentence in document]\n",
    "        if verbose:\n",
    "            print(\"\\nAfter filtering short tokens, long tokens, and stop words:\\n\\n\")\n",
    "            [print(sentence) for sentence in document]\n",
    "            print(\"\\n\\n\", \"-\"*50)\n",
    "        \n",
    "        # Flatten the sentence tokens, so that the document is represented as a list of \n",
    "        # candidate keyphrases\n",
    "        document = [token for sentence in document for token in sentence]\n",
    "        if verbose:\n",
    "            print(\"\\nFinal preprocessed text:\\n\\n\", document, \"\\n\\n\", \"-\"*50)\n",
    "        return document\n",
    "        \n",
    "    \n",
    "    @classmethod\n",
    "    def preprocess(cls, text: List[str], file_names: Optional[List[Path]] = None) -> List[List[str]]:\n",
    "        \"\"\"Preprocess a corpus of raw scholarly text into keyphrase candidates for each of\n",
    "        the documents in the corpus.\n",
    "        \n",
    "        Args:\n",
    "            text:\n",
    "                The corpus as a list of documents as raw text.\n",
    "            file_names:\n",
    "                Optional; A list corrsponding the file names for each document in the corpus.\n",
    "        \n",
    "        Returns:\n",
    "            The corpus as lists of keyphrase candidate tokens for each document.\n",
    "        \"\"\"\n",
    "        if (file_names == None):\n",
    "            text = [cls.preprocess_one(document) for document in text]\n",
    "        else:\n",
    "            text = [cls.preprocess_one(document, file_name) \n",
    "                    for document, file_name in zip(text, file_names)]\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7  en files\n",
      "93  ru files\n"
     ]
    }
   ],
   "source": [
    "print(len(get_file_names(\"../data/raw_txt/2010/en\", \".txt\")), \" en files\")\n",
    "print(len(get_file_names(\"../data/raw_txt/2010/ru\", \".txt\")), \" ru files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- \n",
      "Raw text, before any preprocessing:\n",
      "\n",
      "\n",
      "7. Заключение\n",
      "В данной работе представлен опыт создания\n",
      "и развития лингво-семантических представлений\n",
      "в интеллектуальных информационных системах,\n",
      "разработанных на основе аппарата расширенных\n",
      "семантических сетей (РСС). Аппарат РСС обеспечивает мощные изобразительные возможности для\n",
      "описания всех уровней естественного языка, включая уровень глубинно-семантических представлений, и межъязыковых соответствий. Конкретные\n",
      "лингвистические процессоры, которые были соз-\n",
      "даны на основе этого подхода, прошли определенный эволюционный путь и позволили выработать\n",
      "проектные решения для основных задач текущего\n",
      "этапа — извлечения и обработки содержательных\n",
      "знаний из текстов на естественных языках и сопоставления языковых структур в текстах на различных языках с учетом базовых трансформаций.\n",
      "Проблема извлечения и обработки знаний открывает перспективы развития интеллектуальных\n",
      "направлений компьютерной лингвистики, поскольку ее основной акцент смещен в сторону глубинных\n",
      "представлений языка, в которых используются как\n",
      "грамматические (морфологические и синтаксические), так и семантические атрибуты для описания\n",
      "языковых объектов. Проводимые нами исследования параллельных текстов направлены также на рассмотрение этой проблемы [20]. Центральное место\n",
      "в наших лингвистических исследованиях занимает\n",
      "изучение и формализация процессов трансформации языковых структур, особенно все варианты\n",
      "глагольно-номинативных трансформаций, создание развитых дистрибутивно-трансформационных\n",
      "описаний предикатых структур для рассматриваемых языков.\n",
      "Для задач извлечения знаний и создания систем\n",
      "ИСПАР дистрибутивно-трансформационные описания имеют также особое значение, поскольку таким\n",
      "образом задаются все возможные способы перевода языковых структур в предикатно-аргументные\n",
      "представления, которые затем используются в процедурах обработки знаний.\n",
      "Литература\n",
      "1.\n",
      "\n",
      "\n",
      " --------------------------------------------------\n",
      "\n",
      "After removing the references section, short paragraphs, parenthesized/bracketed text, and text at pagebreaks:\n",
      "\n",
      "\n",
      "7. Заключение В данной работе представлен опыт создания и развития лингво-семантических представлений в интеллектуальных информационных системах, разработанных на основе аппарата расширенных семантических сетей . Аппарат РСС обеспечивает мощные изобразительные возможности для описания всех уровней естественного языка, включая уровень глубинно-семантических представлений, и межъязыковых соответствий. Конкретные лингвистические процессоры, которые были даны на основе этого подхода, прошли определенный эволюционный путь и позволили выработать проектные решения для основных задач текущего этапа — извлечения и обработки содержательных знаний из текстов на естественных языках и сопоставления языковых структур в текстах на различных языках с учетом базовых трансформаций. Проблема извлечения и обработки знаний открывает перспективы развития интеллектуальных направлений компьютерной лингвистики, поскольку ее основной акцент смещен в сторону глубинных представлений языка, в которых используются как грамматические , так и семантические атрибуты для описания языковых объектов. Проводимые нами исследования параллельных текстов направлены также на рассмотрение этой проблемы . Центральное место в наших лингвистических исследованиях занимает изучение и формализация процессов трансформации языковых структур, особенно все варианты глагольно-номинативных трансформаций, создание развитых дистрибутивно-трансформационных описаний предикатых структур для рассматриваемых языков. Для задач извлечения знаний и создания систем ИСПАР дистрибутивно-трансформационные описания имеют также особое значение, поскольку таким образом задаются все возможные способы перевода языковых структур в предикатно-аргументные представления, которые затем используются в процедурах обработки знаний.\n",
      "\n",
      "\n",
      " --------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-16 19:09:35,349 : INFO : collecting all words and their counts\n",
      "2020-07-16 19:09:35,353 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2020-07-16 19:09:35,360 : INFO : collected 256 word types from a corpus of 200 words (unigram + bigrams) and 8 sentences\n",
      "2020-07-16 19:09:35,364 : INFO : using 256 counts as vocab in Phrases<0 vocab, min_count=3, threshold=10.0, max_vocab_size=40000000>\n",
      "2020-07-16 19:09:35,374 : INFO : collecting all words and their counts\n",
      "2020-07-16 19:09:35,378 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2020-07-16 19:09:35,408 : INFO : collected 256 word types from a corpus of 200 words (unigram + bigrams) and 8 sentences\n",
      "2020-07-16 19:09:35,419 : INFO : using 256 counts as vocab in Phrases<0 vocab, min_count=1, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After tokenization:\n",
      "\n",
      "\n",
      "['x', '.', 'заключение', 'в', 'данный', 'работа', 'представлять', 'опыт', 'создание', 'и', 'развитие', 'лингво-семантический', 'представление', 'в', 'интеллектуальный', 'информационный', 'система', ',', 'разрабатывать', 'на', 'основа', 'аппарат', 'расширить', 'семантический', 'сеть', '.', 'аппарат::рсс', 'обеспечивать', 'мощный', 'изобразительный', 'возможность', 'для', 'описание', 'весь', 'уровень', 'естественный', 'язык', ',', 'включая', 'уровень', 'глубинный', '-', 'семантический', 'представление', ',', 'и', 'межъязыковый', 'соответствие', '.', 'конкретный', 'лингвистический', 'процессор', ',', 'который', 'быть', 'давать', 'на', 'основа', 'этот', 'подход', ',', 'проходить', 'определенный', 'эволюционный', 'путь', 'и', 'позволять', 'вырабатывать', 'проектный', 'решение', 'для', 'основной', 'задача', 'текущий', 'этап', '—', 'извлечение', 'и', 'обработка', 'содержательный', 'знание', 'из', 'текст', 'на', 'естественный', 'язык', 'и', 'сопоставление', 'языковой', 'структура', 'в', 'текст', 'на', 'различный', 'язык', 'с', 'учет', 'базовый', 'трансформация', '.', 'проблема', 'извлечение', 'и', 'обработка', 'знание', 'открывать', 'перспектива', 'развитие', 'интеллектуальный', 'направление', 'компьютерный', 'лингвистика', ',', 'поскольку', 'она', 'основной', 'акцент', 'смещать', 'в', 'сторона', 'глубинный', 'представление', 'язык', ',', 'в', 'который', 'использовать', 'как', 'грамматический', ',', 'так', 'и', 'семантический', 'атрибут', 'для', 'описание', 'языковой', 'объект', '.', 'проводить', 'мы', 'исследование', 'параллельный', 'текст', 'направлять', 'также', 'на', 'рассмотрение', 'этот', 'проблема', '.', 'центральный', 'место', 'в', 'наш', 'лингвистический', 'исследование', 'занимать', 'изучение', 'и', 'формализация', 'процесс', 'трансформация', 'языковой', 'структура', ',', 'особенно', 'весь', 'вариант', 'глагольный', '-', 'номинативный', 'трансформация', ',', 'создание', 'развитый', 'дистрибутивно-', 'трансформационный', 'описание', 'предикинать', 'структура', 'для', 'рассматривать', 'язык', '.', 'для', 'задача', 'извлечение', 'знание', 'и', 'создание', 'система', 'испар', 'дистрибутивно-трансформационный', 'описание', 'иметь', 'также', 'особый', 'значение', ',', 'поскольку', 'такой', 'образ', 'задавать', 'весь', 'возможный', 'способ', 'перевод', 'языковой', 'структура', 'в', 'предикатный', '-', 'аргументный', 'представление', ',', 'который', 'затем', 'использовать', 'в', 'процедура', 'обработка', 'знание', '.', '\\n']\n",
      "\n",
      "\n",
      " --------------------------------------------------\n",
      "\n",
      "After removing tokens which do not have at least 1 alphabetical character and are not sentence delimiters:\n",
      "\n",
      "\n",
      "['x', '.', 'заключение', 'в', 'данный', 'работа', 'представлять', 'опыт', 'создание', 'и', 'развитие', 'лингво-семантический', 'представление', 'в', 'интеллектуальный', 'информационный', 'система', 'разрабатывать', 'на', 'основа', 'аппарат', 'расширить', 'семантический', 'сеть', '.', 'аппарат::рсс', 'обеспечивать', 'мощный', 'изобразительный', 'возможность', 'для', 'описание', 'весь', 'уровень', 'естественный', 'язык', 'включая', 'уровень', 'глубинный', 'семантический', 'представление', 'и', 'межъязыковый', 'соответствие', '.', 'конкретный', 'лингвистический', 'процессор', 'который', 'быть', 'давать', 'на', 'основа', 'этот', 'подход', 'проходить', 'определенный', 'эволюционный', 'путь', 'и', 'позволять', 'вырабатывать', 'проектный', 'решение', 'для', 'основной', 'задача', 'текущий', 'этап', 'извлечение', 'и', 'обработка', 'содержательный', 'знание', 'из', 'текст', 'на', 'естественный', 'язык', 'и', 'сопоставление', 'языковой', 'структура', 'в', 'текст', 'на', 'различный', 'язык', 'с', 'учет', 'базовый', 'трансформация', '.', 'проблема', 'извлечение', 'и', 'обработка', 'знание', 'открывать', 'перспектива', 'развитие', 'интеллектуальный', 'направление', 'компьютерный', 'лингвистика', 'поскольку', 'она', 'основной', 'акцент', 'смещать', 'в', 'сторона', 'глубинный', 'представление', 'язык', 'в', 'который', 'использовать', 'как', 'грамматический', 'так', 'и', 'семантический', 'атрибут', 'для', 'описание', 'языковой', 'объект', '.', 'проводить', 'мы', 'исследование', 'параллельный', 'текст', 'направлять', 'также', 'на', 'рассмотрение', 'этот', 'проблема', '.', 'центральный', 'место', 'в', 'наш', 'лингвистический', 'исследование', 'занимать', 'изучение', 'и', 'формализация', 'процесс', 'трансформация', 'языковой', 'структура', 'особенно', 'весь', 'вариант', 'глагольный', 'номинативный', 'трансформация', 'создание', 'развитый', 'дистрибутивно-', 'трансформационный', 'описание', 'предикинать', 'структура', 'для', 'рассматривать', 'язык', '.', 'для', 'задача', 'извлечение', 'знание', 'и', 'создание', 'система', 'испар', 'дистрибутивно-трансформационный', 'описание', 'иметь', 'также', 'особый', 'значение', 'поскольку', 'такой', 'образ', 'задавать', 'весь', 'возможный', 'способ', 'перевод', 'языковой', 'структура', 'в', 'предикатный', 'аргументный', 'представление', 'который', 'затем', 'использовать', 'в', 'процедура', 'обработка', 'знание', '.', '\\n']\n",
      "\n",
      "\n",
      " --------------------------------------------------\n",
      "\n",
      "After adding bigrams and trigrams:\n",
      "\n",
      "\n",
      "['x']\n",
      "['заключение', 'в', 'данный', 'работа', 'представлять', 'опыт', 'создание', 'и', 'развитие', 'лингво-семантический', 'представление', 'в', 'интеллектуальный', 'информационный', 'система', 'разрабатывать', 'на', 'основа', 'аппарат', 'расширить', 'семантический', 'сеть']\n",
      "['аппарат::рсс', 'обеспечивать', 'мощный', 'изобразительный', 'возможность', 'для', 'описание', 'весь', 'уровень', 'естественный язык', 'включая', 'уровень', 'глубинный', 'семантический', 'представление', 'и', 'межъязыковый', 'соответствие']\n",
      "['конкретный', 'лингвистический', 'процессор', 'который', 'быть', 'давать', 'на', 'основа', 'этот', 'подход', 'проходить', 'определенный', 'эволюционный', 'путь', 'и', 'позволять', 'вырабатывать', 'проектный', 'решение', 'для', 'основной', 'задача', 'текущий', 'этап', 'извлечение и обработка', 'содержательный', 'знание', 'из', 'текст', 'на', 'естественный язык', 'и', 'сопоставление', 'языковой структура', 'в', 'текст', 'на', 'различный', 'язык', 'с', 'учет', 'базовый', 'трансформация']\n",
      "['проблема', 'извлечение и обработка', 'знание', 'открывать', 'перспектива', 'развитие', 'интеллектуальный', 'направление', 'компьютерный', 'лингвистика', 'поскольку', 'она', 'основной', 'акцент', 'смещать', 'в', 'сторона', 'глубинный', 'представление', 'язык', 'в', 'который', 'использовать', 'как', 'грамматический', 'так', 'и', 'семантический', 'атрибут', 'для', 'описание', 'языковой', 'объект']\n",
      "['проводить', 'мы', 'исследование', 'параллельный', 'текст', 'направлять', 'также', 'на', 'рассмотрение', 'этот', 'проблема']\n",
      "['центральный', 'место', 'в', 'наш', 'лингвистический', 'исследование', 'занимать', 'изучение', 'и', 'формализация', 'процесс', 'трансформация', 'языковой структура', 'особенно', 'весь', 'вариант', 'глагольный', 'номинативный', 'трансформация', 'создание', 'развитый', 'дистрибутивно-', 'трансформационный', 'описание', 'предикинать', 'структура', 'для', 'рассматривать', 'язык']\n",
      "['для', 'задача', 'извлечение', 'знание', 'и', 'создание', 'система', 'испар', 'дистрибутивно-трансформационный', 'описание', 'иметь', 'также', 'особый', 'значение', 'поскольку', 'такой', 'образ', 'задавать', 'весь', 'возможный', 'способ', 'перевод', 'языковой структура', 'в', 'предикатный', 'аргументный', 'представление', 'который', 'затем', 'использовать', 'в', 'процедура', 'обработка знание']\n",
      "\n",
      "\n",
      " --------------------------------------------------\n",
      "\n",
      "After filtering short tokens, long tokens, and stop words:\n",
      "\n",
      "\n",
      "[]\n",
      "['заключение', 'данный', 'работа', 'представлять', 'опыт', 'создание', 'развитие', 'лингво-семантический', 'представление', 'интеллектуальный', 'информационный', 'система', 'разрабатывать', 'основа', 'аппарат', 'расширить', 'семантический', 'сеть']\n",
      "['аппарат::рсс', 'обеспечивать', 'мощный', 'изобразительный', 'возможность', 'описание', 'весь', 'уровень', 'естественный язык', 'включая', 'уровень', 'глубинный', 'семантический', 'представление', 'межъязыковый', 'соответствие']\n",
      "['конкретный', 'лингвистический', 'процессор', 'давать', 'основа', 'подход', 'проходить', 'определенный', 'эволюционный', 'путь', 'позволять', 'вырабатывать', 'проектный', 'решение', 'основной', 'задача', 'текущий', 'этап', 'извлечение и обработка', 'содержательный', 'знание', 'текст', 'естественный язык', 'сопоставление', 'языковой структура', 'текст', 'различный', 'язык', 'учет', 'базовый', 'трансформация']\n",
      "['проблема', 'извлечение и обработка', 'знание', 'открывать', 'перспектива', 'развитие', 'интеллектуальный', 'направление', 'компьютерный', 'лингвистика', 'поскольку', 'основной', 'акцент', 'смещать', 'сторона', 'глубинный', 'представление', 'язык', 'использовать', 'грамматический', 'семантический', 'атрибут', 'описание', 'языковой', 'объект']\n",
      "['проводить', 'исследование', 'параллельный', 'текст', 'направлять', 'рассмотрение', 'проблема']\n",
      "['центральный', 'место', 'наш', 'лингвистический', 'исследование', 'занимать', 'изучение', 'формализация', 'процесс', 'трансформация', 'языковой структура', 'особенно', 'весь', 'вариант', 'глагольный', 'номинативный', 'трансформация', 'создание', 'развитый', 'дистрибутивно-', 'трансформационный', 'описание', 'предикинать', 'структура', 'рассматривать', 'язык']\n",
      "['задача', 'извлечение', 'знание', 'создание', 'система', 'испар', 'дистрибутивно-трансформационный', 'описание', 'иметь', 'особый', 'значение', 'поскольку', 'образ', 'задавать', 'весь', 'возможный', 'способ', 'перевод', 'языковой структура', 'предикатный', 'аргументный', 'представление', 'затем', 'использовать', 'процедура', 'обработка знание']\n",
      "\n",
      "\n",
      " --------------------------------------------------\n",
      "\n",
      "Final preprocessed text:\n",
      "\n",
      " ['заключение', 'данный', 'работа', 'представлять', 'опыт', 'создание', 'развитие', 'лингво-семантический', 'представление', 'интеллектуальный', 'информационный', 'система', 'разрабатывать', 'основа', 'аппарат', 'расширить', 'семантический', 'сеть', 'аппарат::рсс', 'обеспечивать', 'мощный', 'изобразительный', 'возможность', 'описание', 'весь', 'уровень', 'естественный язык', 'включая', 'уровень', 'глубинный', 'семантический', 'представление', 'межъязыковый', 'соответствие', 'конкретный', 'лингвистический', 'процессор', 'давать', 'основа', 'подход', 'проходить', 'определенный', 'эволюционный', 'путь', 'позволять', 'вырабатывать', 'проектный', 'решение', 'основной', 'задача', 'текущий', 'этап', 'извлечение и обработка', 'содержательный', 'знание', 'текст', 'естественный язык', 'сопоставление', 'языковой структура', 'текст', 'различный', 'язык', 'учет', 'базовый', 'трансформация', 'проблема', 'извлечение и обработка', 'знание', 'открывать', 'перспектива', 'развитие', 'интеллектуальный', 'направление', 'компьютерный', 'лингвистика', 'поскольку', 'основной', 'акцент', 'смещать', 'сторона', 'глубинный', 'представление', 'язык', 'использовать', 'грамматический', 'семантический', 'атрибут', 'описание', 'языковой', 'объект', 'проводить', 'исследование', 'параллельный', 'текст', 'направлять', 'рассмотрение', 'проблема', 'центральный', 'место', 'наш', 'лингвистический', 'исследование', 'занимать', 'изучение', 'формализация', 'процесс', 'трансформация', 'языковой структура', 'особенно', 'весь', 'вариант', 'глагольный', 'номинативный', 'трансформация', 'создание', 'развитый', 'дистрибутивно-', 'трансформационный', 'описание', 'предикинать', 'структура', 'рассматривать', 'язык', 'задача', 'извлечение', 'знание', 'создание', 'система', 'испар', 'дистрибутивно-трансформационный', 'описание', 'иметь', 'особый', 'значение', 'поскольку', 'образ', 'задавать', 'весь', 'возможный', 'способ', 'перевод', 'языковой структура', 'предикатный', 'аргументный', 'представление', 'затем', 'использовать', 'процедура', 'обработка знание'] \n",
      "\n",
      " --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Peek at the transformation of the last paragraph of the first document in the corpus\n",
    "# as it progresses through the preprocessing steps\n",
    "file_name = Path(\"C:/Users/Whyve/Projects/auto-keyphrase-extraction-ru/data/raw_txt/2010/ru/\",\n",
    "                 \"эволюция-лингво-семантических-представлений-в-интеллектуальных-системах-на-основе-расширенных-семантических-сетей-p-205.txt\")\n",
    "text = \"\\n\\n\".join(get_text(file_name).split(\"\\n\\n\")[-6:-3])\n",
    "text = ScholarlyPreprocessor.preprocess_one(text, file_name, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the entire corpus\n",
    "file_names = get_file_names(\"../data/raw_txt/2010/ru\", \".txt\")\n",
    "corpus = [get_text(name) for name in file_names]\n",
    "corpus = ScholarlyPreprocessor.preprocess(corpus, file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
